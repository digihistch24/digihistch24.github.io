@inproceedings{ares_oliveira_deep_2019,
	address = {Utrecht, Netherlands},
	title = {A deep learning approach to {Cadastral} {Computing}},
	url = {https://dev.clariah.nl/files/dh2019/boa/0691.html},
	language = {en},
	urldate = {2020-05-22},
	booktitle = {{DH2019}},
	author = {Ares Oliveira, Sofia and di Lenardo, Isabella and Tourenc, Bastien and Kaplan, Frederic},
	month = jul,
	year = {2019},
	file = {Snapshot:/Users/remipetitpierre/Zotero/storage/72ARJNID/0691.html:text/html},
}

@article{lelo_analysing_2020,
	title = {Analysing spatial relationships through the urban cadastre of nineteenth-century {Rome}},
	volume = {47},
	doi = {10.1017/S0963926820000188},
	number = {3},
	journal = {Urban History},
	author = {Lelo, Keti},
	year = {2020},
	note = {Publisher: Cambridge University Press},
	pages = {467--487},
}

@article{di_lenardo_approche_2021,
	title = {Une approche computationnelle du cadastre napoléonien de {Venise}},
	volume = {3},
	doi = {0.4000/revuehn.1786},
	language = {fr},
	journal = {Humanités numériques},
	author = {di Lenardo, Isabella and Barman, Raphaël and Pardini, Federica and Kaplan, Frédéric},
	month = may,
	year = {2021},
}

@book{hennet_recueil_1811,
	address = {Paris},
	title = {Recueil méthodique des lois, décrets, règlemens, instructions et décisions sur le cadastre de la {France}},
	publisher = {Imprimerie impériale},
	author = {Hennet, Albert-Joseph-Ulpien},
	year = {1811},
}

@book{clergeot_cent_2007,
	address = {Paris},
	title = {Cent millions de parcelles en {France}: 1807, un cadastre pour l'{Empire}},
	isbn = {978-2-9519379-5-6},
	publisher = {Editions Publi-Topex},
	author = {Clergeot, Pierre},
	year = {2007},
}

@misc{berney_plan_1831,
	title = {Plan cadastral par l'arpenteur {Abraham} {Berney}},
	author = {Berney, Abraham},
	year = {1831},
}

@misc{melotte_plans_1722,
	title = {Plans du territoire de {Lausanne}},
	author = {Melotte, Sebastian and Perey, Claude},
	year = {1722},
}

@inproceedings{chen_vectorization_2021,
	address = {Cham},
	title = {Vectorization of {Historical} {Maps} {Using} {Deep} {Edge} {Filtering} and {Closed} {Shape} {Extraction}},
	isbn = {978-3-030-86337-1},
	booktitle = {Document {Analysis} and {Recognition} – {ICDAR} 2021},
	publisher = {Springer International Publishing},
	author = {Chen, Yizi and Carlinet, Edwin and Chazalon, Joseph and Mallet, Clément and Duménieu, Bertrand and Perret, Julien},
	editor = {Lladós, Josep and Lopresti, Daniel and Uchida, Seiichi},
	year = {2021},
	pages = {510--525},
}

@misc{noauthor_mmsegmentation_2022,
    author = {{OpenMMLab}},
	title = {{MMSegmentation}: {OpenMMLab} {Semantic} {Segmentation} {Toolbox} and {Benchmark}},
	shorttitle = {{MMSegmentation}},
	url = {https://github.com/open-mmlab/mmsegmentation},
	publisher = {OpenMMLab},
	year = {2022},
}

@book{soulier_instructions_1827,
	address = {Lausanne},
	edition = {Frères Blanchard},
	title = {Instructions données par le {Département} des {Finances} pour la levée des plans et l'établissement du cadastre, ensuite de la décision du {Conseil} d'{Etat} du 6 décembre 1826},
	author = {Soulier and Berdez},
	year = {1827},
}

@article{petitpierre_effective_2023,
	title = {Effective annotation for the automatic vectorization of cadastral maps},
	issn = {2055-7671},
	url = {https://doi.org/10.1093/llc/fqad006},
	doi = {10.1093/llc/fqad006},
	abstract = {The great potential brought by large-scale data in the humanities is still hindered by the time and technicality required for making documents digitally intelligible. Within urban studies, historical cadasters have been hitherto largely under-explored despite their informative value. Powerful and generic technologies, based on neural networks, to automate the vectorization of historical maps have recently become available. However, the transfer of these technologies is hampered by the scarcity of interdisciplinary exchanges and a lack of practical literature destinated to humanities scholars, especially on the key step of the pipeline: the annotation. In this article, we propose a set of practical recommendations based on empirical findings on document annotation and automatic vectorization, focusing on the example case of historical cadasters. Our recommendations are generic and easily applicable, based on a solid experience on concrete and diverse projects.},
	journal = {Digital Scholarship in the Humanities},
	author = {Petitpierre, Remi and Guhennec, Paul},
	month = mar,
	year = {2023},
	note = {\_eprint: https://academic.oup.com/dsh/advance-article-pdf/doi/10.1093/llc/fqad006/49480836/fqad006.pdf},
}

@article{petitpierre_endtoend_2023,
	title = {An end-to-end pipeline for historical censuses processing},
	issn = {1433-2825},
	url = {https://doi.org/10.1007/s10032-023-00428-9},
	doi = {10.1007/s10032-023-00428-9},
	abstract = {Censuses are structured documents of great value for social and demographic history, which became widespread from the nineteenth century on. However, the plurality of formats and the natural variability of historical data make their extraction arduous and often lead to ungeneric recognition algorithms. We propose an end-to-end processing pipeline, based on optimization, in an attempt to reduce the number of free parameters. The layout analysis is based on semantic segmentation using neural networks for a generic recognition of the explicit column structure. The implicit row structure is deduced directly from the position of the text segments. The handwritten text detection is complemented by an intelligent framing method which significantly improves the quality of the HTR. In the end, we propose to combine several post-correction approaches, neural networks, and language models, to further improve the performance. Ultimately, our flexible methods make it possible to accurately detect more than 98\% of the columns and 88\% of the rows, despite the lack of graphical separator and the diversity of formats. Thanks to various reframing and post-correction strategies, HTR results reach the excellent performance of 3.44\% character error rate on these noisy nineteenth century data. In total, more than 18,831 pages were extracted in 72 censuses over a century. This large historical dataset, as well as training data, is made open-access and released along with this article.},
	language = {en},
	urldate = {2023-06-12},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {Petitpierre, Remi and Kramer, Marion and Rappo, Lucas},
	month = mar,
	year = {2023},
	keywords = {Handwritten text recognition, Historical document processing, Layout analysis, OCR post-correction, Tabular document understanding},
	file = {Full Text PDF:/Users/remipetitpierre/Zotero/storage/EU5N5ZI6/Petitpierre et al. - 2023 - An end-to-end pipeline for historical censuses pro.pdf:application/pdf},
}

@inproceedings{petitpierre_recartographier_2023,
	title = {Recartographier l'espace napoléonien},
	url = {https://hal.science/hal-04109214},
	abstract = {Le cadastre napoléonien est une source historique relativement homogène et largement répandue. Cela rend une approche computationnelle particulièrement pertinente. Dans cette étude, nous proposons une méthode de reconnaissance et de vectorisation automatique des cartes cadastrales à la pointe de la technologie. Nous démontrons son efficacité sur le cas lausannois et proposons des méthodes d'analyse congruentes.},
	language = {fr},
	urldate = {2023-06-13},
	author = {Petitpierre, Remi and Rappo, Lucas and Lenardo, Isabella di},
	month = jun,
	year = {2023},
	file = {Full Text PDF:/Users/remipetitpierre/Zotero/storage/J5IEWQE8/Petitpierre et al. - 2023 - Recartographier l'espace napoléonien.pdf:application/pdf},
}

@article{vaienti_machine-learning-enhanced_2023,
	title = {Machine-{Learning}-{Enhanced} {Procedural} {Modeling} for {4D} {Historical} {Cities} {Reconstruction}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/15/13/3352},
	doi = {10.3390/rs15133352},
	abstract = {The generation of 3D models depicting cities in the past holds great potential for documentation and educational purposes. However, it is often hindered by incomplete historical data and the specialized expertise required. To address these challenges, we propose a framework for historical city reconstruction. By integrating procedural modeling techniques and machine learning models within a Geographic Information System (GIS) framework, our pipeline allows for effective management of spatial data and the generation of detailed 3D models. We developed an open-source Python module that fills gaps in 2D GIS datasets and directly generates 3D models up to LOD 2.1 from GIS files. The use of the CityJSON format ensures interoperability and accommodates the specific needs of historical models. A practical case study using footprints of the Old City of Jerusalem between 1840 and 1940 demonstrates the creation, completion, and 3D representation of the dataset, highlighting the versatility and effectiveness of our approach. This research contributes to the accessibility and accuracy of historical city models, providing tools for the generation of informative 3D models. By incorporating machine learning models and maintaining the dynamic nature of the models, we ensure the possibility of supporting ongoing updates and refinement based on newly acquired data. Our procedural modeling methodology offers a streamlined and open-source solution for historical city reconstruction, eliminating the need for additional software and increasing the usability and practicality of the process.},
	language = {en},
	number = {13},
	urldate = {2023-08-23},
	journal = {Remote Sensing},
	author = {Vaienti, Beatrice and Petitpierre, Rémi and di Lenardo, Isabella and Kaplan, Frédéric},
	month = jan,
	year = {2023},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D building modeling, 4D city modeling, 4D urban reconstruction, CityJSON, GIS, HBIM, historical maps, machine learning, procedural modeling, vectorization},
	pages = {3352},
	file = {Full Text PDF:/Users/remipetitpierre/Zotero/storage/TYKFFH3B/Vaienti et al. - 2023 - Machine-Learning-Enhanced Procedural Modeling for .pdf:application/pdf},
}

@misc{cheng_masked-attention_2022,
	title = {Masked-attention {Mask} {Transformer} for {Universal} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/2112.01527},
	doi = {10.48550/arXiv.2112.01527},
	abstract = {Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit},
	month = jun,
	year = {2022},
	note = {arXiv:2112.01527 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/remipetitpierre/Zotero/storage/Z5QQIIRH/Cheng et al. - 2022 - Masked-attention Mask Transformer for Universal Im.pdf:application/pdf;arXiv.org Snapshot:/Users/remipetitpierre/Zotero/storage/RBC4LDQJ/2112.html:text/html},
}

@misc{petitpierre_1805-1898_2023,
	title = {1805-1898 {Census} {Records} of {Lausanne} : a {Long} {Digital} {Dataset} for {Demographic} {History}},
	shorttitle = {1805-1898 {Census} {Records} of {Lausanne}},
	url = {https://zenodo.org/records/7711640},
	doi = {10.5281/zenodo.7711640},
	abstract = {Context. This historical dataset stems from the project of automatic extraction of 72 census records of Lausanne, Switzerland. The complete dataset covers a century of historical demography in Lausanne (1805-1898), which corresponds to 18,831 pages, and nearly 6 million cells. Content. The data published in this repository correspond to a first release, i.e. a diachronic slice of one register every 8 to 9 years. Unfortunately, the remaining data are currently under embargo. Their publication will take place as soon as possible, and at the latest by the end of 2023. In the meantime, the data presented here correspond to a large subset of 2,844 pages, which already allows to investigate most research hypotheses. Description. The population censuses, digitized by the Archives of the city of Lausanne, continuously cover the evolution of the population in Lausanne throughout the 19th century, starting in 1805, with only one long interruption from 1814 to 1831. Highly detailed, they are an invaluable source for studying migration, economic and social history, and traces of cultural exchanges not only with Bern, but also with France and Italy. Indeed, the system of tracing family origin, specific to Switzerland, allows to follow the migratory movements of families long before the censuses appeared. The bourgeoisie is also an essential economic tracer. In addition, censuses extensively describe the organization of the social fabric into family nuclei, around which gravitate various boarders, workers, servants or apprentices, often living in the same apartment with the family. Production. The structure and richness of censuses have also provided an opportunity to develop automatic methods for processing structured documents. The processing of censuses includes several steps, from the identification of text segments to the restructuring of information as digital tabular data, through Handwritten Text Recognition and the automatic segmentation of the structure using neural networks. Please note that the detailed extraction methodology, as well as the complete evaluation of performance and reliability is published in: Petitpierre R., Rappo L., Kramer M. (2023). An end-to-end pipeline for historical censuses processing. International Journal on Document Analysis and Recognition (IJDAR). doi: 10.1007/s10032-023-00428-9 Data structure. The data are structured in rows and columns, with each row corresponding to a household. Multiple entries in the same column for a single household are separated by vertical bars ⟨{\textbar}⟩. The center point ⟨·⟩ indicates an empty entry. For some columns (e.g., street name, house number, owner name), an empty entry indicates that the last non-empty value should be carried over. The page number is in the last column. Liability. The data presented here are not curated nor verified. They are the raw results of the extraction, the reliability of which was thoroughly assessed in the above-mentioned publication. We insist on the fact that for any reuse of this data for research purposes, the implementation of an appropriate methodology is necessary. This may typically include string distance heuristics, or statistical methodologies to deal with noise and uncertainty.},
	language = {en},
	urldate = {2023-11-30},
	author = {Petitpierre, Remi and Kramer, Marion and Rappo, Lucas and di Lenardo, Isabella},
	month = mar,
	year = {2023},
	file = {Snapshot:/Users/remipetitpierre/Zotero/storage/KVX5JJ7I/7711640.html:text/html},
}

@inproceedings{petitpierre_mapping_2023,
	address = {Graz, Austria},
	title = {Mapping {Memes} in the {Napoleonic} {Cadastre}: {Expanding} {Frontiers} in {Memetics}},
	shorttitle = {Mapping {Memes} in the {Napoleonic} {Cadastre}},
	doi = {10.5281/zenodo.8107916},
	abstract = {We develop a practical computational methodology based on memes theory for studying historical cartography. We also propose to investigate the limits of replication fidelity, and the link between memes and their expression, through a case study on the Napoleonic cadastre},
	booktitle = {Digital {Humanities} 2023: {Book} of {Abstracts}},
	publisher = {Zenodo},
	author = {Petitpierre, Remi},
	year = {2023},
	keywords = {computer vision, cultural analytics, history of cartography, memes theory, stylometry},
	pages = {3},
}

@misc{goderle_ai-driven_2023,
	title = {{AI}-driven {Structure} {Detection} and {Information} {Extraction} from {Historical} {Cadastral} {Maps} ({Early} 19th {Century} {Franciscean} {Cadastre} in the {Province} of {Styria}) and {Current} {High}-resolution {Satellite} and {Aerial} {Imagery} for {Remote} {Sensing}},
	url = {http://arxiv.org/abs/2312.07560},
	doi = {10.48550/arXiv.2312.07560},
	abstract = {Cadastres from the 19th century are a complex as well as rich source for historians and archaeologists, whose use presents them with great challenges. For archaeological and historical remote sensing, we have trained several Deep Learning models, CNNs as well as Vision Transformers, to extract large-scale data from this knowledge representation. We present the principle results of our work here and we present a the demonstrator of our browser-based tool that allows researchers and public stakeholders to quickly identify spots that featured buildings in the 19th century Franciscean Cadastre. The tool not only supports scholars and fellow researchers in building a better understanding of the settlement history of the region of Styria, it also helps public administration and fellow citizens to swiftly identify areas of heightened sensibility with regard to the cultural heritage of the region.},
	urldate = {2024-07-09},
	publisher = {arXiv},
	author = {Göderle, Wolfgang and Macher, Christian and Mauthner, Katrin and Pimas, Oliver and Rampetsreiter, Fabian},
	month = dec,
	year = {2023},
	note = {arXiv:2312.07560 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/remipetitpierre/Zotero/storage/Q8NABVLT/Göderle et al. - 2023 - AI-driven Structure Detection and Information Extr.pdf:application/pdf;arXiv.org Snapshot:/Users/remipetitpierre/Zotero/storage/2FXVSSI6/2312.html:text/html},
}

@article{arkin_efficiently_1991,
	title = {An efficiently computable metric for comparing polygonal shapes},
	volume = {13},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/75509},
	doi = {10.1109/34.75509},
	abstract = {A method for comparing polygons that is a metric, invariant under translation, rotation, and change of scale, reasonably easy to compute, and intuitive is presented. The method is based on the L/sub 2/ distance between the turning functions of the two polygons. It works for both convex and nonconvex polygons and runs in time O(mn log mn), where m is the number of vertices in one polygon and n is the number of vertices in the other. Some examples showing that the method produces answers that are intuitively reasonable are presented.{\textless}{\textgreater}},
	number = {3},
	urldate = {2024-07-10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Arkin, E.M. and Chew, L.P. and Huttenlocher, D.P. and Kedem, K. and Mitchell, J.S.B.},
	month = mar,
	year = {1991},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computer science, Computer vision, Cost function, Geometry, Image recognition, Machine vision, Rotation measurement, Shape measurement, Solid modeling, Turning},
	pages = {209--216},
	file = {IEEE Xplore Abstract Record:/Users/remipetitpierre/Zotero/storage/WQM7NGNI/75509.html:text/html;IEEE Xplore Full Text PDF:/Users/remipetitpierre/Zotero/storage/D334RY7F/Arkin et al. - 1991 - An efficiently computable metric for comparing pol.pdf:application/pdf},
}

@misc{deluz_plans_1886,
	title = {Plans de {Lausanne}},
	author = {Deluz, Louis},
	year = {1886},
}

@article{rickli_lausanne_1978,
	title = {Lausanne: deux siècles de devenir urbain},
	volume = {51},
	number = {12},
	journal = {Habitation},
	author = {Rickli, Jean Daniel},
	year = {1978},
}

@book{grandjean_lausanne_1982,
	address = {Basel},
	edition = {Birkhäuser},
	series = {Monuments d'art et d'histoire du {Canton} de {Vaud}},
	title = {Lausanne: villages, hameaux et maisons de l’ancienne campagne lausannoise},
	volume = {4},
	author = {Grandjean, Marcel},
	year = {1982},
}

@article{follin_detection_2021,
	title = {Détection automatique des parcelles sur les plans napoléoniens : comparaison de deux méthodes},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2736-2337},
	shorttitle = {Détection automatique des parcelles sur les plans napoléoniens},
	url = {https://journals.openedition.org/revuehn/1779},
	doi = {10.4000/revuehn.1779},
	abstract = {Le cadastre napoléonien fournit la description la plus détaillée du territoire français dans sa globalité au xixe siècle. Il contient une mine d’informations (structure des parcelles cadastrales, numéro des parcelles, tracé des rivières et des routes, toponymie, bâti…) auxquelles les chercheurs en sciences humaines (archéologues, historiens, urbanistes…) s’intéressent de plus en plus, notamment pour mener des analyses historiques du territoire. Les travaux présentés ici s’inscrivent dans cette perspective et proposent une chaîne de traitement semi-automatique permettant de vectoriser, géoréférencer et assembler des planches scannées du cadastre ancien afin de construire une base de données multi-époques. Dans cet article, nous abordons plus particulièrement les solutions adoptées pour l’étape de vectorisation automatique des parcelles en comparant deux méthodes, l’une basée sur la transformée de Hough probabiliste (THP) et l’autre sur l’algorithme Line Segment Detector (LSD). Après avoir précisé comment ces méthodes ont été implémentées pour notre étude, nous présentons les résultats obtenus, qui montrent la supériorité de la méthode LSD sur la méthode THP pour les planches cadastrales les plus anciennes.},
	language = {fr},
	number = {3},
	urldate = {2024-07-24},
	journal = {Humanités numériques},
	author = {Follin, Jean-Michel and Simonetto, Élisabeth and Chalais, Anthony},
	month = may,
	year = {2021},
	note = {Number: 3
Publisher: Humanistica},
	keywords = {humanités numériques spatialisées, imagerie, numérisation},
	file = {Full Text PDF:/Users/remipetitpierre/Zotero/storage/S3PQVW78/Follin et al. - 2021 - Détection automatique des parcelles sur les plans .pdf:application/pdf},
}
