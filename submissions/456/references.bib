
@inproceedings{ares_oliveira_deep_2019,
	address = {Utrecht, Netherlands},
	title = {A deep learning approach to {Cadastral} {Computing}},
	url = {https://dev.clariah.nl/files/dh2019/boa/0691.html},
	language = {en},
	urldate = {2020-05-22},
	booktitle = {{DH2019}},
	author = {Ares Oliveira, Sofia and di Lenardo, Isabella and Tourenc, Bastien and Kaplan, Frederic},
	month = jul,
	year = {2019},
	file = {Snapshot:/Users/remipetitpierre/Zotero/storage/72ARJNID/0691.html:text/html},
}

@article{lelo_analysing_2020,
	title = {Analysing spatial relationships through the urban cadastre of nineteenth-century {Rome}},
	volume = {47},
	doi = {10.1017/S0963926820000188},
	number = {3},
	journal = {Urban History},
	author = {Lelo, Keti},
	year = {2020},
	note = {Publisher: Cambridge University Press},
	pages = {467--487},
}

@article{di_lenardo_approche_2021,
	title = {Une approche computationnelle du cadastre napoléonien de {Venise}},
	volume = {3},
	doi = {0.4000/revuehn.1786},
	language = {fr},
	journal = {Humanités numériques},
	author = {di Lenardo, Isabella and Barman, Raphaël and Pardini, Federica and Kaplan, Frédéric},
	month = may,
	year = {2021},
}

@book{hennet_recueil_1811,
	address = {Paris},
	title = {Recueil méthodique des lois, décrets, règlemens, instructions et décisions sur le cadastre de la {France}},
	publisher = {Imprimerie impériale},
	author = {Hennet, Albert-Joseph-Ulpien},
	year = {1811},
}

@book{clergeot_cent_2007,
	address = {Paris},
	title = {Cent millions de parcelles en {France}: 1807, un cadastre pour l'{Empire}},
	isbn = {978-2-9519379-5-6},
	publisher = {Editions Publi-Topex},
	author = {Clergeot, Pierre},
	year = {2007},
}

@misc{berney_plan_1831,
	title = {Plan cadastral par l'arpenteur {Abraham} {Berney}},
	author = {Berney, Abraham},
	year = {1831},
}

@misc{melotte_plans_1722,
	title = {Plans du territoire de {Lausanne}},
	author = {Melotte, Sebastian and Perey, Claude},
	year = {1722},
}

@inproceedings{chen_vectorization_2021,
	address = {Cham},
	title = {Vectorization of {Historical} {Maps} {Using} {Deep} {Edge} {Filtering} and {Closed} {Shape} {Extraction}},
	isbn = {978-3-030-86337-1},
	booktitle = {Document {Analysis} and {Recognition} – {ICDAR} 2021},
	publisher = {Springer International Publishing},
	author = {Chen, Yizi and Carlinet, Edwin and Chazalon, Joseph and Mallet, Clément and Duménieu, Bertrand and Perret, Julien},
	editor = {Lladós, Josep and Lopresti, Daniel and Uchida, Seiichi},
	year = {2021},
	pages = {510--525},
}

@misc{noauthor_mmsegmentation_2022,
	title = {{MMSegmentation}: {OpenMMLab} {Semantic} {Segmentation} {Toolbox} and {Benchmark}},
	shorttitle = {{MMSegmentation}},
	url = {https://github.com/open-mmlab/mmsegmentation},
	publisher = {OpenMMLab},
	year = {2022},
}

@book{soulier_instructions_1827,
	address = {Lausanne},
	edition = {Frères Blanchard},
	title = {Instructions données par le {Département} des {Finances} pour la levée des plans et l'établissement du cadastre, ensuite de la décision du {Conseil} d'{Etat} du 6 décembre 1826},
	author = {Soulier and Berdez},
	year = {1827},
}

@article{petitpierre_effective_2023,
	title = {Effective annotation for the automatic vectorization of cadastral maps},
	issn = {2055-7671},
	url = {https://doi.org/10.1093/llc/fqad006},
	doi = {10.1093/llc/fqad006},
	abstract = {The great potential brought by large-scale data in the humanities is still hindered by the time and technicality required for making documents digitally intelligible. Within urban studies, historical cadasters have been hitherto largely under-explored despite their informative value. Powerful and generic technologies, based on neural networks, to automate the vectorization of historical maps have recently become available. However, the transfer of these technologies is hampered by the scarcity of interdisciplinary exchanges and a lack of practical literature destinated to humanities scholars, especially on the key step of the pipeline: the annotation. In this article, we propose a set of practical recommendations based on empirical findings on document annotation and automatic vectorization, focusing on the example case of historical cadasters. Our recommendations are generic and easily applicable, based on a solid experience on concrete and diverse projects.},
	journal = {Digital Scholarship in the Humanities},
	author = {Petitpierre, Remi and Guhennec, Paul},
	month = mar,
	year = {2023},
	note = {\_eprint: https://academic.oup.com/dsh/advance-article-pdf/doi/10.1093/llc/fqad006/49480836/fqad006.pdf},
}

@article{petitpierre_endtoend_2023,
	title = {An end-to-end pipeline for historical censuses processing},
	issn = {1433-2825},
	url = {https://doi.org/10.1007/s10032-023-00428-9},
	doi = {10.1007/s10032-023-00428-9},
	abstract = {Censuses are structured documents of great value for social and demographic history, which became widespread from the nineteenth century on. However, the plurality of formats and the natural variability of historical data make their extraction arduous and often lead to ungeneric recognition algorithms. We propose an end-to-end processing pipeline, based on optimization, in an attempt to reduce the number of free parameters. The layout analysis is based on semantic segmentation using neural networks for a generic recognition of the explicit column structure. The implicit row structure is deduced directly from the position of the text segments. The handwritten text detection is complemented by an intelligent framing method which significantly improves the quality of the HTR. In the end, we propose to combine several post-correction approaches, neural networks, and language models, to further improve the performance. Ultimately, our flexible methods make it possible to accurately detect more than 98\% of the columns and 88\% of the rows, despite the lack of graphical separator and the diversity of formats. Thanks to various reframing and post-correction strategies, HTR results reach the excellent performance of 3.44\% character error rate on these noisy nineteenth century data. In total, more than 18,831 pages were extracted in 72 censuses over a century. This large historical dataset, as well as training data, is made open-access and released along with this article.},
	language = {en},
	urldate = {2023-06-12},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {Petitpierre, Remi and Kramer, Marion and Rappo, Lucas},
	month = mar,
	year = {2023},
	keywords = {Layout analysis, Historical document processing, Handwritten text recognition, OCR post-correction, Tabular document understanding},
	file = {Full Text PDF:/Users/remipetitpierre/Zotero/storage/EU5N5ZI6/Petitpierre et al. - 2023 - An end-to-end pipeline for historical censuses pro.pdf:application/pdf},
}

@inproceedings{petitpierre_recartographier_2023,
	title = {Recartographier l'espace napoléonien},
	url = {https://hal.science/hal-04109214},
	abstract = {Le cadastre napoléonien est une source historique relativement homogène et largement répandue. Cela rend une approche computationnelle particulièrement pertinente. Dans cette étude, nous proposons une méthode de reconnaissance et de vectorisation automatique des cartes cadastrales à la pointe de la technologie. Nous démontrons son efficacité sur le cas lausannois et proposons des méthodes d'analyse congruentes.},
	language = {fr},
	urldate = {2023-06-13},
	author = {Petitpierre, Remi and Rappo, Lucas and Lenardo, Isabella di},
	month = jun,
	year = {2023},
	file = {Full Text PDF:/Users/remipetitpierre/Zotero/storage/J5IEWQE8/Petitpierre et al. - 2023 - Recartographier l'espace napoléonien.pdf:application/pdf},
}

@article{vaienti_machine-learning-enhanced_2023,
	title = {Machine-{Learning}-{Enhanced} {Procedural} {Modeling} for {4D} {Historical} {Cities} {Reconstruction}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/15/13/3352},
	doi = {10.3390/rs15133352},
	abstract = {The generation of 3D models depicting cities in the past holds great potential for documentation and educational purposes. However, it is often hindered by incomplete historical data and the specialized expertise required. To address these challenges, we propose a framework for historical city reconstruction. By integrating procedural modeling techniques and machine learning models within a Geographic Information System (GIS) framework, our pipeline allows for effective management of spatial data and the generation of detailed 3D models. We developed an open-source Python module that fills gaps in 2D GIS datasets and directly generates 3D models up to LOD 2.1 from GIS files. The use of the CityJSON format ensures interoperability and accommodates the specific needs of historical models. A practical case study using footprints of the Old City of Jerusalem between 1840 and 1940 demonstrates the creation, completion, and 3D representation of the dataset, highlighting the versatility and effectiveness of our approach. This research contributes to the accessibility and accuracy of historical city models, providing tools for the generation of informative 3D models. By incorporating machine learning models and maintaining the dynamic nature of the models, we ensure the possibility of supporting ongoing updates and refinement based on newly acquired data. Our procedural modeling methodology offers a streamlined and open-source solution for historical city reconstruction, eliminating the need for additional software and increasing the usability and practicality of the process.},
	language = {en},
	number = {13},
	urldate = {2023-08-23},
	journal = {Remote Sensing},
	author = {Vaienti, Beatrice and Petitpierre, Rémi and di Lenardo, Isabella and Kaplan, Frédéric},
	month = jan,
	year = {2023},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {machine learning, GIS, historical maps, CityJSON, 3D building modeling, 4D city modeling, 4D urban reconstruction, HBIM, procedural modeling, vectorization},
	pages = {3352},
	file = {Full Text PDF:/Users/remipetitpierre/Zotero/storage/TYKFFH3B/Vaienti et al. - 2023 - Machine-Learning-Enhanced Procedural Modeling for .pdf:application/pdf},
}

@misc{cheng_masked-attention_2022,
	title = {Masked-attention {Mask} {Transformer} for {Universal} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/2112.01527},
	doi = {10.48550/arXiv.2112.01527},
	abstract = {Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit},
	month = jun,
	year = {2022},
	note = {arXiv:2112.01527 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/remipetitpierre/Zotero/storage/Z5QQIIRH/Cheng et al. - 2022 - Masked-attention Mask Transformer for Universal Im.pdf:application/pdf;arXiv.org Snapshot:/Users/remipetitpierre/Zotero/storage/RBC4LDQJ/2112.html:text/html},
}

@misc{petitpierre_1805-1898_2023,
	title = {1805-1898 {Census} {Records} of {Lausanne} : a {Long} {Digital} {Dataset} for {Demographic} {History}},
	shorttitle = {1805-1898 {Census} {Records} of {Lausanne}},
	url = {https://zenodo.org/records/7711640},
	doi = {10.5281/zenodo.7711640},
	abstract = {Context. This historical dataset stems from the project of automatic extraction of 72 census records of Lausanne, Switzerland. The complete dataset covers a century of historical demography in Lausanne (1805-1898), which corresponds to 18,831 pages, and nearly 6 million cells. Content. The data published in this repository correspond to a first release, i.e. a diachronic slice of one register every 8 to 9 years. Unfortunately, the remaining data are currently under embargo. Their publication will take place as soon as possible, and at the latest by the end of 2023. In the meantime, the data presented here correspond to a large subset of 2,844 pages, which already allows to investigate most research hypotheses. Description. The population censuses, digitized by the Archives of the city of Lausanne, continuously cover the evolution of the population in Lausanne throughout the 19th century, starting in 1805, with only one long interruption from 1814 to 1831. Highly detailed, they are an invaluable source for studying migration, economic and social history, and traces of cultural exchanges not only with Bern, but also with France and Italy. Indeed, the system of tracing family origin, specific to Switzerland, allows to follow the migratory movements of families long before the censuses appeared. The bourgeoisie is also an essential economic tracer. In addition, censuses extensively describe the organization of the social fabric into family nuclei, around which gravitate various boarders, workers, servants or apprentices, often living in the same apartment with the family. Production. The structure and richness of censuses have also provided an opportunity to develop automatic methods for processing structured documents. The processing of censuses includes several steps, from the identification of text segments to the restructuring of information as digital tabular data, through Handwritten Text Recognition and the automatic segmentation of the structure using neural networks. Please note that the detailed extraction methodology, as well as the complete evaluation of performance and reliability is published in: Petitpierre R., Rappo L., Kramer M. (2023). An end-to-end pipeline for historical censuses processing. International Journal on Document Analysis and Recognition (IJDAR). doi: 10.1007/s10032-023-00428-9 Data structure. The data are structured in rows and columns, with each row corresponding to a household. Multiple entries in the same column for a single household are separated by vertical bars ⟨{\textbar}⟩. The center point ⟨·⟩ indicates an empty entry. For some columns (e.g., street name, house number, owner name), an empty entry indicates that the last non-empty value should be carried over. The page number is in the last column. Liability. The data presented here are not curated nor verified. They are the raw results of the extraction, the reliability of which was thoroughly assessed in the above-mentioned publication. We insist on the fact that for any reuse of this data for research purposes, the implementation of an appropriate methodology is necessary. This may typically include string distance heuristics, or statistical methodologies to deal with noise and uncertainty.},
	language = {en},
	urldate = {2023-11-30},
	author = {Petitpierre, Remi and Kramer, Marion and Rappo, Lucas and di Lenardo, Isabella},
	month = mar,
	year = {2023},
	file = {Snapshot:/Users/remipetitpierre/Zotero/storage/KVX5JJ7I/7711640.html:text/html},
}

@inproceedings{petitpierre_mapping_2023,
	address = {Graz, Austria},
	title = {Mapping {Memes} in the {Napoleonic} {Cadastre}: {Expanding} {Frontiers} in {Memetics}},
	shorttitle = {Mapping {Memes} in the {Napoleonic} {Cadastre}},
	doi = {10.5281/zenodo.8107916},
	abstract = {We develop a practical computational methodology based on memes theory for studying historical cartography. We also propose to investigate the limits of replication fidelity, and the link between memes and their expression, through a case study on the Napoleonic cadastre},
	booktitle = {Digital {Humanities} 2023: {Book} of {Abstracts}},
	publisher = {Zenodo},
	author = {Petitpierre, Remi},
	year = {2023},
	keywords = {computer vision, cultural analytics, history of cartography, memes theory, stylometry},
	pages = {3},
}

@misc{goderle_ai-driven_2023,
	title = {{AI}-driven {Structure} {Detection} and {Information} {Extraction} from {Historical} {Cadastral} {Maps} ({Early} 19th {Century} {Franciscean} {Cadastre} in the {Province} of {Styria}) and {Current} {High}-resolution {Satellite} and {Aerial} {Imagery} for {Remote} {Sensing}},
	url = {http://arxiv.org/abs/2312.07560},
	doi = {10.48550/arXiv.2312.07560},
	abstract = {Cadastres from the 19th century are a complex as well as rich source for historians and archaeologists, whose use presents them with great challenges. For archaeological and historical remote sensing, we have trained several Deep Learning models, CNNs as well as Vision Transformers, to extract large-scale data from this knowledge representation. We present the principle results of our work here and we present a the demonstrator of our browser-based tool that allows researchers and public stakeholders to quickly identify spots that featured buildings in the 19th century Franciscean Cadastre. The tool not only supports scholars and fellow researchers in building a better understanding of the settlement history of the region of Styria, it also helps public administration and fellow citizens to swiftly identify areas of heightened sensibility with regard to the cultural heritage of the region.},
	urldate = {2024-07-09},
	publisher = {arXiv},
	author = {Göderle, Wolfgang and Macher, Christian and Mauthner, Katrin and Pimas, Oliver and Rampetsreiter, Fabian},
	month = dec,
	year = {2023},
	note = {arXiv:2312.07560 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/remipetitpierre/Zotero/storage/Q8NABVLT/Göderle et al. - 2023 - AI-driven Structure Detection and Information Extr.pdf:application/pdf;arXiv.org Snapshot:/Users/remipetitpierre/Zotero/storage/2FXVSSI6/2312.html:text/html},
}

@article{arkin_efficiently_1991,
	title = {An efficiently computable metric for comparing polygonal shapes},
	volume = {13},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/75509},
	doi = {10.1109/34.75509},
	abstract = {A method for comparing polygons that is a metric, invariant under translation, rotation, and change of scale, reasonably easy to compute, and intuitive is presented. The method is based on the L/sub 2/ distance between the turning functions of the two polygons. It works for both convex and nonconvex polygons and runs in time O(mn log mn), where m is the number of vertices in one polygon and n is the number of vertices in the other. Some examples showing that the method produces answers that are intuitively reasonable are presented.{\textless}{\textgreater}},
	number = {3},
	urldate = {2024-07-10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Arkin, E.M. and Chew, L.P. and Huttenlocher, D.P. and Kedem, K. and Mitchell, J.S.B.},
	month = mar,
	year = {1991},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Solid modeling, Image recognition, Machine vision, Geometry, Computer vision, Computer science, Cost function, Rotation measurement, Shape measurement, Turning},
	pages = {209--216},
	file = {IEEE Xplore Abstract Record:/Users/remipetitpierre/Zotero/storage/WQM7NGNI/75509.html:text/html;IEEE Xplore Full Text PDF:/Users/remipetitpierre/Zotero/storage/D334RY7F/Arkin et al. - 1991 - An efficiently computable metric for comparing pol.pdf:application/pdf},
}

@misc{deluz_plans_1886,
	title = {Plans de {Lausanne}},
	author = {Deluz, Louis},
	year = {1886},
}

@article{rickli_lausanne_1978,
	title = {Lausanne: deux siècles de devenir urbain},
	volume = {51},
	number = {12},
	journal = {Habitation},
	author = {Rickli, Jean Daniel},
	year = {1978},
}

@book{grandjean_lausanne_1982,
	address = {Basel},
	edition = {Birkhäuser},
	series = {Monuments d'art et d'histoire du {Canton} de {Vaud}},
	title = {Lausanne: villages, hameaux et maisons de l’ancienne campagne lausannoise},
	volume = {4},
	author = {Grandjean, Marcel},
	year = {1982},
}

@article{follin_detection_2021,
	title = {Détection automatique des parcelles sur les plans napoléoniens : comparaison de deux méthodes},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2736-2337},
	shorttitle = {Détection automatique des parcelles sur les plans napoléoniens},
	url = {https://journals.openedition.org/revuehn/1779},
	doi = {10.4000/revuehn.1779},
	abstract = {Le cadastre napoléonien fournit la description la plus détaillée du territoire français dans sa globalité au xixe siècle. Il contient une mine d’informations (structure des parcelles cadastrales, numéro des parcelles, tracé des rivières et des routes, toponymie, bâti…) auxquelles les chercheurs en sciences humaines (archéologues, historiens, urbanistes…) s’intéressent de plus en plus, notamment pour mener des analyses historiques du territoire. Les travaux présentés ici s’inscrivent dans cette perspective et proposent une chaîne de traitement semi-automatique permettant de vectoriser, géoréférencer et assembler des planches scannées du cadastre ancien afin de construire une base de données multi-époques. Dans cet article, nous abordons plus particulièrement les solutions adoptées pour l’étape de vectorisation automatique des parcelles en comparant deux méthodes, l’une basée sur la transformée de Hough probabiliste (THP) et l’autre sur l’algorithme Line Segment Detector (LSD). Après avoir précisé comment ces méthodes ont été implémentées pour notre étude, nous présentons les résultats obtenus, qui montrent la supériorité de la méthode LSD sur la méthode THP pour les planches cadastrales les plus anciennes.},
	language = {fr},
	number = {3},
	urldate = {2024-07-24},
	journal = {Humanités numériques},
	author = {Follin, Jean-Michel and Simonetto, Élisabeth and Chalais, Anthony},
	month = may,
	year = {2021},
	note = {Number: 3
Publisher: Humanistica},
	keywords = {humanités numériques spatialisées, numérisation, imagerie},
	file = {Full Text PDF:/Users/remipetitpierre/Zotero/storage/S3PQVW78/Follin et al. - 2021 - Détection automatique des parcelles sur les plans .pdf:application/pdf},
}

@inproceedings{burgermeister_enabling_2020,
	address = {Vienna, Austria},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Enabling the {Scholarly} {Discourse} of the {Future}: {Versioning} {RDF} {Data} in the {Digital}               {Humanities}},
	volume = {3110},
	shorttitle = {Enabling the {Scholarly} {Discourse} of the {Future}},
	url = {https://ceur-ws.org/Vol-3110/#paper1},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {Graph {Technologies} in the {Humanities} - {Proceedings} 2020},
	publisher = {CEUR},
	author = {Bürgermeister, Martina},
	editor = {Andrews, Tara and Diehr, Franziska and Efer, Thomas and Kuczera, Andreas and Zundert, Joris van},
	month = feb,
	year = {2020},
	note = {ISSN: 1613-0073},
	pages = {1},
	file = {Full Text PDF:/Users/remipetitpierre/Zotero/storage/9ZIFNUQD/Bürgermeister - 2020 - Enabling the Scholarly Discourse of the Future Ve.pdf:application/pdf},
}

@incollection{gibbs_hermeneutics_2013,
	title = {The {Hermeneutics} of {Data} and {Historical} {Writing}},
	url = {https://www.jstor.org/stable/j.ctv65sx57.18},
	abstract = {Ongoing digitization of primary sources and the proliferation of born-digital documents are making it easier for historians to engage with vast amounts of research material. As a result, historical scholarship increasingly depends on our interactions with data, from battling the hidden algorithms of Google Book Search to text mining a hand-curated set of full-text documents. Even though methods for exploring and interacting with data have begun to permeate historical research, historians’ writing has largely remained mired in traditional forms and conventions. This essay discusses some new ways in which historians might rethink the nature of historical writing as both a},
	urldate = {2024-07-25},
	booktitle = {Writing {History} in the {Digital} {Age}},
	publisher = {University of Michigan Press},
	author = {Gibbs, Fred and Owens, Trevor},
	editor = {Dougherty, Jack and Nawrotzki, Kristen},
	year = {2013},
	doi = {10.2307/j.ctv65sx57.18},
	pages = {159--170},
	file = {JSTOR Full Text PDF:/Users/remipetitpierre/Zotero/storage/CRUNCIKJ/Gibbs and Owens - 2013 - The Hermeneutics of Data and Historical Writing.pdf:application/pdf},
}

@book{gormley_elasticsearch_2015,
	address = {Beijing ; Sebastopol, CA},
	edition = {1st edition},
	title = {Elasticsearch: {The} {Definitive} {Guide}: {A} {Distributed} {Real}-{Time} {Search} and {Analytics} {Engine}},
	isbn = {978-1-4493-5854-9},
	shorttitle = {Elasticsearch},
	abstract = {Whether you need full-text search or real-time analytics of structured data―or both―the Elasticsearch distributed search engine is an ideal way to put your data to work. This practical guide not only shows you how to search, analyze, and explore data with Elasticsearch, but also helps you deal with the complexities of human language, geolocation, and relationships.  If you’re a newcomer to both search and distributed systems, you’ll quickly learn how to integrate Elasticsearch into your application. More experienced users will pick up lots of advanced techniques. Throughout the book, you’ll follow a problem-based approach to learn why, when, and how to use Elasticsearch features. Understand how Elasticsearch interprets data in your documents Index and query your data to take advantage of search concepts such as relevance and word proximity Handle human language through the effective use of analyzers and queries Summarize and group data to show overall trends, with aggregations and analytics Use geo-points and geo-shapes―Elasticsearch’s approaches to geolocation Model your data to take advantage of Elasticsearch’s horizontal scalability Learn how to configure and monitor your cluster in production},
	language = {English},
	publisher = {O'Reilly Media},
	author = {Gormley, Clinton and Tong, Zachary},
	month = mar,
	year = {2015},
}

@article{kaplan_big_2017,
	title = {Big {Data} of the {Past}},
	volume = {4},
	issn = {2297-2668},
	url = {https://www.frontiersin.org/journals/digital-humanities/articles/10.3389/fdigh.2017.00012/full},
	doi = {10.3389/fdigh.2017.00012},
	abstract = {{\textless}p{\textgreater}{\textless}italic{\textgreater}Big Data{\textless}/italic{\textgreater} is not a new phenomenon. History is punctuated by regimes of {\textless}italic{\textgreater}data acceleration{\textless}/italic{\textgreater}, characterized by feelings of information overload accompanied by periods of social transformation and the invention of new technologies. During these moments, private organizations, administrative powers, and sometimes isolated individuals have produced important datasets, organized following a logic that is often subsequently superseded but was at the time, nevertheless, coherent. To be translated into relevant sources of information about our past, these document series need to be redocumented using contemporary paradigms. The intellectual, methodological, and technological challenges linked to this translation process are the central subject of this article.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-07-25},
	journal = {Frontiers in Digital Humanities},
	author = {Kaplan, Frédéric and di Lenardo, Isabella},
	month = may,
	year = {2017},
	note = {Publisher: Frontiers},
	keywords = {big data, Cultural heritage digitization, digital history, Digital Humanities, public history},
	file = {Full Text:/Users/remipetitpierre/Zotero/storage/TKSHZBFM/Kaplan and di Lenardo - 2017 - Big Data of the Past.pdf:application/pdf},
}

@article{maryl_recognition_2023,
	title = {Recognition and {Assessment} of {Digital} {Scholarly} {Outputs} in the {Humanities}},
	copyright = {Copyright (c) 2023 Maciej Maryl},
	issn = {2387-3086},
	url = {https://septentrio.uit.no/index.php/SCS/article/view/7151},
	doi = {10.7557/5.7151},
	abstract = {Watch VIDEO.
In recent years we have observed an increase in digital practices and outputs in scholarship, which should be understood as a standard evolution of scholarly practices to take advantage of digital technologies. And although written genres, such as the monograph or essay, remain dominant in the humanities, the range of technological possibilities allow scholars to redefine those forms of expression and enrich them with other media or genres. However, as the opening example showed, this innovation is not supported by the assessment system, or even sometimes takes place in spite of it. A change in attitude requires recognition of three key aspects of digital humanities work: (1) its interdisciplinarity in borrowing tools and methods from ICT or social sciences; (2) the new research practices which should be recognised as valid scholarly work; (3) innovative scholarly outputs that go beyond the traditional genres but provide valid research results.
This presentation discusses the recommendations of the ALLEA E-Humanities Working Group with regards to the assessment of novel scholarly communication genres in the humanities. The work is based on the group’s previous report, Sustainable and FAIR Data Sharing in the Humanities, which provided recommendations on data practices in the humanities. The current focus is on attuning institutional policies to emerging scholarly needs in connection to current research assessment reform (CoARA). The recommendations are prepared in close cooperation with stakeholders and the research community. It underwent an open consultation whereby we collected more than 200 comments from the public. The final draft is under preparation and will be published in fall 2023.
The ALLEA E-Humanities Working Group recommendations are meant to serve as guidance for institutions and evaluators to embrace innovative outputs in the humanities and thus create space for their development. The Working Group has prepared tailored recommendations which could be divided into two main groups. First, the group focuses on the cross-cutting issues pertinent to digital practices in the humanities, which are (1) linking studies with underlying data, (2) updating and versioning of the outputs, (3) collaboration and authorship, (4) training and competence building, and (5) reviewing. Next, we discuss particular case studies of innovative outputs where cross-cutting issues manifest themselves, such as digital scholarly editions, extended publications, databases, visualisations, code and blogs. The overall conclusions provide some general remarks on recognising and evaluating digital practices in the humanities.},
	language = {en},
	number = {1},
	urldate = {2024-07-25},
	journal = {Septentrio Conference Series},
	author = {Maryl, Maciej},
	month = sep,
	year = {2023},
	note = {Number: 1},
	keywords = {digital humanities},
}

@inproceedings{neudecker_making_2016,
	title = {Making {Europe}'s {Historical} {Newspapers} {Searchable}},
	url = {https://ieeexplore.ieee.org/document/7490152/similar#similar},
	doi = {10.1109/DAS.2016.83},
	abstract = {This paper provides a rare glimpse into the overall approach for the refinement, i.e. the enrichment of scanned historical newspapers with text and layout recognition, in the Europeana Newspapers project. Within three years, the project processed more than 10 million pages of historical newspapers from 12 national and major libraries to produce the largest open access and fully searchable text collection of digital historical newspapers in Europe. In this, a wide variety of legal, logistical, technical and other challenges were encountered. After introducing the background issues in newspaper digitization in Europe, the paper discusses the technical aspects of refinement in greater detail. It explains what decisions were taken in the design of the large-scale processing workflow to address these challenges, what were the results produced and what were identified as best practices.},
	urldate = {2024-07-25},
	booktitle = {2016 12th {IAPR} {Workshop} on {Document} {Analysis} {Systems} ({DAS})},
	author = {Neudecker, Clemens and Antonacopoulos, Apostolos},
	month = apr,
	year = {2016},
	keywords = {Character recognition, digital libraries, Europe, historical newspapers, large-scale digitisation, Layout, layout analysis, Libraries, Metadata, optical character recognition, Optical character recognition software, Text analysis},
	pages = {405--410},
	file = {IEEE Xplore Abstract Record:/Users/remipetitpierre/Zotero/storage/CTSYQJAD/similar.html:text/html;Submitted Version:/Users/remipetitpierre/Zotero/storage/RICA4RKY/Neudecker and Antonacopoulos - 2016 - Making Europe's Historical Newspapers Searchable.pdf:application/pdf},
}

@article{traub_measuring_2020,
	title = {Measuring {Tool} {Bias} \& {Improving} {Data} {Quality} for {Digital} {Humanities} {Research}},
	copyright = {Open Access (free)},
	url = {https://dspace.library.uu.nl/handle/1874/396185},
	abstract = {Cultural heritage institutions increasingly make their collections digitally available. Consequently, users of digital archives need to familiarize themselves with new kinds of different digital tools. This is particularly true for humanities scholars who include results of their analyses in their publications. Judging whether insights derived from these analyses constitute a real trend or whether a potential conclusion is just an artifact of the tools used, can be difficult. To correct errors in data, human input is in many cases still indispensable. Since experts are expensive, we conducted a study showing how crowdsourcing tasks can be designed to allow lay users to contribute information at the expert level to increase the number and quality of descriptions of collection items. However, to improve the quality of their data effectively, data custodians need to understand the (search) tasks their users perform and the level of trustworthiness they expect from the results. Through interviews with historians, we studied their use of digital archives and classified typical research tasks and their requirements for data quality. Most archives provide, at best, very generic information about the data quality of their digitized collections. Humanities scholars, however, need to be able to assess how data quality and inherent bias within tools influence their research tasks. Therefore, they need specific information on the data quality of the subcollection used and the biases the tools provided may have introduced into the analyses. We studied whether access to a historic newspaper archive is biased, and which types of documents benefit from, or are disadvantaged, by the bias. Using real and simulated search queries and page view data of real users, we investigated how well typical retrievability studies reflect the users' experience. We discovered large differences in the characteristics of the query sets and in the results for different parameter settings of the experiments. Within digital archives, OCR errors are a prevalent data quality issue. Since these are relatively easy to spot, it has caused some concern about the trustworthiness of results based on digitized documents. We evaluated the impact of OCR quality on retrieval tasks, and studied the effect of manually improving (parts of) a collection on retrievability bias. The insights we gained helped us understanding researchers' needs better. Our work provides a small number of examples, which demonstrate that data quality and tool bias are real concerns to the Digital Humanities community. To address these challenges, intense multidisciplinary exchange is required: • Humanities scholars need to enhance the awareness that software tools and data sets are not free of bias and develop skills to detect and evaluate biases and their impact on research tasks. Guidelines should be developed that help scholars to perform tool criticism. • Tool developers need to be more transparent and provide sufficient information about their tools to allow the task-based evaluation of their tools' performance. • Data custodians need to make as much information about their collection available as possible. This should include which tools were used in the digitization process, in addition to both the limitations of the provided data and infrastructure used. The goal should be a mutual understanding of each others' assumptions, approaches and requirements and more transparency concerning the use of tools in the processing of data. This will help scholars to develop effective methods of digital tool criticism to critically assess the impact of existing tools on their (re-)search results and to communicate on an equal footing with tool developers on how to develop future versions, which better suit their needs.},
	language = {en},
	number = {2020-09},
	urldate = {2024-07-25},
	journal = {SIKS Dissertation Series},
	author = {Traub, Myriam Christine},
	month = may,
	year = {2020},
	note = {Accepted: 2020-05-06T16:15:13Z
ISBN: 9783000653643
Publisher: Utrecht University},
	file = {Full Text PDF:/Users/remipetitpierre/Zotero/storage/97X6QLHF/Traub - 2020 - Measuring Tool Bias & Improving Data Quality for D.pdf:application/pdf},
}

@phdthesis{musso_standardising_2023,
	address = {Lausanne, Switzerland},
	type = {Master's project},
	title = {Standardising ownership information from the {Napoleonic} {Cadastre} of 1808 {Venice}: methods and findings in the first database creation},
	school = {EPFL},
	author = {Musso, Carlo},
	year = {2023},
}
